version: '3.8'
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./data:/data
      - ./faiss_index:/faiss_index
    environment:
      - INDEX_PATH=/faiss_index
      - DATA_DIR=/data
      - LLM_PROVIDER=${LLM_PROVIDER:-huggingface}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-phi3:instruct}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - HF_API_TOKEN=${HF_API_TOKEN:-}  # supply via .env (not committed)
      - HF_MODEL=${HF_MODEL:-google/flan-t5-base}
      - API_KEY=${API_KEY:-}
  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
    environment:
      - VITE_BACKEND_URL=http://localhost:8000
      - VITE_API_KEY=${API_KEY:-}
    command: ["npm","run","dev","--","--host","0.0.0.0"]
  # ollama:  # Optional - run Ollama separately per their install instructions
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
